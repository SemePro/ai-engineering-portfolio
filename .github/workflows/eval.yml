name: LLM Evaluation

on:
  workflow_dispatch:
    inputs:
      suite:
        description: 'Evaluation suite to run'
        required: true
        default: 'basic'
        type: choice
        options:
          - basic
  schedule:
    # Run weekly on Sundays at midnight
    - cron: '0 0 * * 0'

jobs:
  run-evaluation:
    name: Run LLM Evaluation
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: ./llm-eval-harness
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: './llm-eval-harness/requirements.txt'
      
      - name: Install dependencies
        run: pip install -r requirements.txt
      
      - name: Run Evaluation Suite
        id: eval
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          python -m src.cli run \
            --suite ./suites/${{ github.event.inputs.suite || 'basic' }}.json \
            --out ./runs/ci_run_${{ github.run_id }}.json \
            --fail-on-regression
        continue-on-error: true
      
      - name: Upload Results
        uses: actions/upload-artifact@v4
        with:
          name: eval-results-${{ github.run_id }}
          path: ./llm-eval-harness/runs/ci_run_${{ github.run_id }}.json
          retention-days: 30
      
      - name: Check for Regression
        if: steps.eval.outcome == 'failure'
        run: |
          echo "::error::LLM Evaluation detected a regression!"
          exit 1
